# -*- coding: utf-8 -*-
"""Perineural_Invasion_Detection_OSCC

Automatically generated by Colab.

Original file is located at
   
"""

from google.colab import drive
drive.mount('')

import os
import pandas as pd
import cv2
import torch
import torchvision
import torch.nn as nn
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models import efficientnet_b4
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import matplotlib.pyplot as plt

import os
import pandas as pd
import cv2
import torch
import torchvision
import torch.nn as nn
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torch.optim.lr_scheduler import StepLR
import matplotlib.pyplot as plt
import shutil
from google.colab import files


def convert_to_grayscale(input_folder, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    for filename in os.listdir(input_folder):
        file_path = os.path.join(input_folder, filename)
        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
            img = cv2.imread(file_path)
            grayscale_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            output_path = os.path.join(output_folder, filename)
            cv2.imwrite(output_path, grayscale_img)
            print(f"Converted {filename} to grayscale.")


def extract_annotations(annotated_folder, class_label='pni', visualize=True):
    img_folder = ''
    output_folder = ''

    data = []
    for img_name in os.listdir(annotated_folder):
        if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):
            img_path = os.path.join(annotated_folder, img_name)
            img = cv2.imread(img_path)
            if img is None:
                print(f"Warning: {img_name} could not be loaded.")
                continue
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            drawn_img = img.copy()

            for cnt in contours:
                x, y, w, h = cv2.boundingRect(cnt)
                if w > 5 and h > 5:
                    data.append({
                        'image_name': img_name,
                        'x_min': x,
                        'y_min': y,
                        'width': w,
                        'height': h,
                        'class': class_label
                    })
                    if visualize:
                        cv2.rectangle(drawn_img, (x, y), (x + w, y + h), (0, 255, 0), 2)

            if visualize:
                plt.figure(figsize=(8, 8))
                plt.imshow(cv2.cvtColor(drawn_img, cv2.COLOR_BGR2RGB))
                plt.title(f"Bounding Boxes: {img_name}")
                plt.axis('off')
                plt.show()

    df = pd.DataFrame(data)
    df.to_csv('pni_csv', index=False)
    print("âœ… Bounding boxes extracted and saved to 'pni_detection_annotations.csv'.")


class PNIDataset(Dataset):
    def __init__(self, csv_file, img_folder, transforms=None):
        self.annotations = pd.read_csv(csv_file)
        self.img_folder = img_folder
        self.transforms = transforms
        self.image_names = self.annotations['image_name'].unique()

    def __len__(self):
        return len(self.image_names)

    def __getitem__(self, idx):
        img_name = self.image_names[idx]
        img_path = os.path.join(self.img_folder, img_name)
        img = cv2.imread(img_path)
        if img is None:
            print(f"Warning: {img_name} could not be loaded.")
            return None, None
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        boxes = []
        records = self.annotations[self.annotations['image_name'] == img_name]
        for _, row in records.iterrows():
            x_min = row['x_min']
            y_min = row['y_min']
            x_max = x_min + row['width']
            y_max = y_min + row['height']
            boxes.append([x_min, y_min, x_max, y_max])

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)

        target = {}
        target['boxes'] = boxes
        target['labels'] = labels

        img = img / 255.0
        img = torch.tensor(img, dtype=torch.float).permute(2, 0, 1)

        if self.transforms:
            img = self.transforms(img)

        return img, target


def get_model(num_classes):
    model = fasterrcnn_resnet50_fpn(pretrained=True)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    return model


def train():
    output_folder = 
    annotation_csv = 'pni_detection_annotations.csv'

    dataset = PNIDataset(annotation_csv, output_folder, transforms=None)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = get_model(num_classes=2)
    model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)

    # Learning Rate Scheduler (StepLR)
    scheduler = StepLR(optimizer, step_size=10, gamma=0.7)

    num_epochs = 50
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0
        for imgs, targets in dataloader:
            if imgs is None:
                continue
            imgs = [img.to(device) for img in imgs if img is not None]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            loss_dict = model(imgs, targets)
            losses = sum(loss for loss in loss_dict.values())

            optimizer.zero_grad()
            losses.backward()

            # Gradient Clipping (to prevent exploding gradients)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)

            optimizer.step()
            epoch_loss += losses.item()

        # Step the scheduler
        scheduler.step()

        print(f"Epoch [{epoch + 1}/{num_epochs}] Loss: {epoch_loss:.4f}")

    torch.save(model.state_dict(), 'pni_detector.pth')
    print("âœ… Model trained and saved.")


def predict_and_visualize(model_path, test_img_folder):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = get_model(num_classes=2)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    test_imgs = [img for img in os.listdir(test_img_folder) if img.lower().endswith(('jpg', 'jpeg', 'png'))]
    for img_name in test_imgs:
        img_path = os.path.join(test_img_folder, img_name)
        img = cv2.imread(img_path)
        if img is None:
            print(f"Warning: {img_name} could not be loaded.")
            continue
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_tensor = torch.tensor(img_rgb / 255.0, dtype=torch.float).permute(2, 0, 1).unsqueeze(0).to(device)

        with torch.no_grad():
            prediction = model(img_tensor)

        pred_boxes = prediction[0]['boxes'].cpu().numpy()
        scores = prediction[0]['scores'].cpu().numpy()

        for box, score in zip(pred_boxes, scores):
            if score > 0.5:
                x1, y1, x2, y2 = box.astype(int)
                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(img, f"PNI: {score:.2f}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36, 255, 12), 2)

        plt.figure(figsize=(10, 10))
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        plt.axis('off')
        plt.title(f"Predictions: {img_name}")
        plt.show()


# ğŸš€ Step to run the training and prediction
extract_annotations('')
train()

predict_and_visualize('pni_detector.pth', '')

from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np

def compute_iou(box1, box2):
    """Compute IoU between two bounding boxes."""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    intersection = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)
    area_box1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)
    area_box2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)
    union = area_box1 + area_box2 - intersection

    iou = intersection / union if union != 0 else 0
    return iou


def evaluate_model(model_path, test_folder, annotation_csv, iou_threshold=0.5):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = get_model(num_classes=2)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    annotations = pd.read_csv(annotation_csv)
    image_names = annotations['image_name'].unique()

    TP, FP, FN = 0, 0, 0

    for img_name in image_names:
        # Ground Truth Boxes
        gt_boxes = []
        records = annotations[annotations['image_name'] == img_name]
        for _, row in records.iterrows():
            x1 = row['x_min']
            y1 = row['y_min']
            x2 = x1 + row['width']
            y2 = y1 + row['height']
            gt_boxes.append([x1, y1, x2, y2])

        img_path = os.path.join(test_folder, img_name)
        img = cv2.imread(img_path)
        if img is None:
            continue
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_tensor = torch.tensor(img_rgb / 255.0, dtype=torch.float).permute(2, 0, 1).unsqueeze(0).to(device)

        with torch.no_grad():
            prediction = model(img_tensor)

        pred_boxes = prediction[0]['boxes'].cpu().numpy()
        scores = prediction[0]['scores'].cpu().numpy()

        pred_boxes_filtered = [box for box, score in zip(pred_boxes, scores) if score > 0.5]

        matched_gt = set()
        for pred_box in pred_boxes_filtered:
            matched = False
            for i, gt_box in enumerate(gt_boxes):
                if i in matched_gt:
                    continue
                iou = compute_iou(pred_box, gt_box)
                if iou >= iou_threshold:
                    TP += 1
                    matched_gt.add(i)
                    matched = True
                    break
            if not matched:
                FP += 1

        FN += len(gt_boxes) - len(matched_gt)

    precision = TP / (TP + FP + 1e-6)
    recall = TP / (TP + FN + 1e-6)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)

    print(f"ğŸ“Š Evaluation Metrics:")
    print(f"âœ” True Positives: {TP}")
    print(f"âŒ False Positives: {FP}")
    print(f"â— False Negatives: {FN}")
    print(f"ğŸ“ Precision: {precision:.4f}")
    print(f"ğŸ“ Recall: {recall:.4f}")
    print(f"ğŸ¯ F1 Score: {f1:.4f}")

evaluate_model('pni_detector.pth', '')

